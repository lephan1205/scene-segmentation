{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to get the data\n",
    "PATH = os.path.join(os.getcwd(), \"data_dir\")\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_movies(path=PATH):\n",
    "    \"\"\"Load .pkl movie files\"\"\"\n",
    "    filenames = glob.glob(os.path.join(PATH, \"tt*.pkl\"))\n",
    "    movies = []\n",
    "    for fn in filenames:\n",
    "        try:\n",
    "            with open(fn, 'rb') as fin:\n",
    "                movies.append(pickle.load(fin))\n",
    "        except EOFError:\n",
    "            break\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = fetch_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, train_size=60):\n",
    "    \"\"\"Split data into train and test sets\"\"\"\n",
    "    # For stable output across runs\n",
    "    np.random.seed(42)\n",
    "    # Shuffle indices\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    train_indices = shuffled_indices[:train_size]\n",
    "    test_indices = shuffled_indices[train_size:]\n",
    "    train_set = [data[i] for i in train_indices]\n",
    "    test_set = [data[i] for i in test_indices]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and test data sets\n",
    "movies = fetch_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split movies into training and validation sets\n",
    "# movies_train, movies_val = split_train_test(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longest movie length\n",
    "# max_movie_length = max([movie['place'].shape[0] for movie in movies])\n",
    "# max_movie_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def transform_movies(movies, features=['place', 'cast', 'action', 'audio'], pad_len=4000):\n",
    "    \"\"\"\n",
    "    Unroll the given features by column and separate features from labels.\n",
    "    Then pad the sequences in each movie to the length of the longest movie.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    # Unroll the features\n",
    "    for movie in movies: \n",
    "        row = torch.cat([movie[feat] for feat in features], dim=1)\n",
    "        X.append(row.numpy())\n",
    "        # Pre-pad the label since its length is N-1\n",
    "        labels = movie['scene_transition_boundary_ground_truth']\n",
    "        labels = torch.cat([torch.tensor([False]), labels])\n",
    "        Y.append(labels.numpy())\n",
    "    # Pad the sequences\n",
    "    X_padded = pad_sequences(X, maxlen=pad_len, padding='post', dtype='float32')\n",
    "    Y_padded = pad_sequences(Y, value=False, maxlen=pad_len, padding='post')\n",
    "    return X_padded, Y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform training and validation sets\n",
    "# X_train, y_train = transform_movies(movies_train)\n",
    "# X_val, y_val = transform_movies(movies_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, transform the entire dataset \n",
    "X, y = transform_movies(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_THRESHOLD = 0.95\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy') > ACCURACY_THRESHOLD):\n",
    "            print(\"\\nReached 95% accuracy, so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2048 + 512 + 512 + 512\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad_predictions(movies, yhat_probs):\n",
    "    \"\"\"Truncate the padded predictions to movie's original length\"\"\"\n",
    "    imdb_lengths = [(movie['imdb_id'], movie['place'].shape[0]) for movie in movies]\n",
    "    yhat_dict = dict()\n",
    "    for (imdb, length), yhat in zip(imdb_lengths, yhat_probs):\n",
    "        yhat = yhat[1:length]\n",
    "        yhat_dict[imdb] = yhat\n",
    "    return yhat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(yhat_unpadded_dict, path=PATH):\n",
    "    for imdb in yhat_unpadded_dict.keys():\n",
    "        # Load existing pkl movie file\n",
    "        filename = os.path.join(PATH, imdb + \".pkl\")\n",
    "        try:\n",
    "            x = pickle.load(open(filename, \"rb\"))\n",
    "            x['scene_transition_boundary_prediction'] = yhat_unpadded_dict[imdb].flatten()\n",
    "            pickle.dump(x, open(filename, \"wb\"))\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(n_neurons=32, input_shape=[4000]):\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=n_neurons,\n",
    "                                                               input_shape=input_shape,\n",
    "                                                               return_sequences=True)),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='sigmoid'))])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_clf = build_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 - 125s - loss: 0.6985 - accuracy: 0.7822\n",
      "Epoch 2/5\n",
      "2/2 - 124s - loss: 0.6401 - accuracy: 0.9351\n",
      "Epoch 3/5\n",
      "2/2 - 108s - loss: 0.6095 - accuracy: 0.9623\n",
      "\n",
      "Reached 95% accuracy, so cancelling training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc0e2583a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_clf.fit(X, y, epochs=NUM_EPOCHS, callbacks=[myCallback()], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_yhat = lstm_clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_yhat_unpadded = unpad_predictions(movies, lstm_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions(lstm_yhat_unpadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_clf = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=256,\n",
    "#                                                        input_shape=[INPUT_DIM],\n",
    "#                                                        return_sequences=True)),\n",
    "#     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "# ])\n",
    "\n",
    "# lstm_clf.compile(loss='binary_crossentropy',\n",
    "#               optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# lstm_clf.fit(X, \n",
    "#           y, \n",
    "#           epochs=1, \n",
    "#           callbacks=[myCallback()],\n",
    "#           verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 WaveNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wave_net(input_shape=[None, INPUT_DIM], filters1=20, filters2=10, kern1=2, kern2=1, padding='same'):\n",
    "    wave_model = tf.keras.models.Sequential()\n",
    "    wave_model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for rate in (1, 2, 4, 8) * 2:\n",
    "        wave_model.add(tf.keras.layers.Conv1D(filters=filters1, \n",
    "                                              kernel_size=kern1, \n",
    "                                              padding='same',\n",
    "                                              activation='relu', dilation_rate=rate))\n",
    "    wave_model.add(tf.keras.layers.Conv1D(filters=filters2, kernel_size=kern2))\n",
    "    wave_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    wave_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\n",
    "                  metrics=['accuracy'])\n",
    "    return wave_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_clf = build_wave_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 - 43s - loss: 0.7007 - accuracy: 0.6144\n",
      "Epoch 2/5\n",
      "2/2 - 28s - loss: 0.6988 - accuracy: 0.6151\n",
      "Epoch 3/5\n",
      "2/2 - 29s - loss: 0.6978 - accuracy: 0.6157\n",
      "Epoch 4/5\n",
      "2/2 - 14s - loss: 0.6971 - accuracy: 0.6164\n",
      "Epoch 5/5\n",
      "2/2 - 11s - loss: 0.6965 - accuracy: 0.6172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc5f4987e50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_clf.fit(X, y, epochs=NUM_EPOCHS, callbacks=[myCallback()], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_hat = wave_clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_hat_unpadded2 = unpad_predictions(movies, wave_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
