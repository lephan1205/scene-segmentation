{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Segmentation\n",
    "\n",
    "#### Author: Le Phan\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Setup](#section2)\n",
    "3. [Get the data](#section3)\n",
    "4. [Data processing](#section4)\n",
    "5. [Training LSTM and WaveNet](#section5)\n",
    "6. [Hyperparameters tuning](#section6)\n",
    "7. [Performance evaluation](#section7)\n",
    "\n",
    "<a id='intro'></a>\n",
    "## 1. Introduction\n",
    "\n",
    "Scenes are important part of storytelling in movies. Detecting semantic scene changes involve understanding the interactions between actors and their environments. The task for this project is to build a machine learning system that detect semantic changes in movie scenes. First, let us define a few vocabularies. A movie is a sequence of _shots_ and _scenes_ and they are quite different. A __shot__ is series of frames captured by a camera for an uninterrupted period of time. A __scene__ is a plot-based semantic unit that is made up of a series of shots.\n",
    "\n",
    "The data is a set of 64 `<imbd id>.pkl` files provided by [eluv.io](https://eluv.io). Each file is a movie containing the following information:\n",
    "* Movie-level: the movie's IMBD identification.\n",
    "* Shot-level: four features (`place`, `cast`, `action`, and `audio`). These features are two-dimensional tensors extracted according to the encoding methods found in [(Rao et al.)](https://arxiv.org/pdf/2004.02678.pdf). The first dimension is the number of shots in the movie. The second dimension are 2048, 512, 512, 512, respectively.\n",
    "* Scene-level:\n",
    "    - Ground truth (`scene_transition_boundary_ground_truth`) which is a boolean vector labeling scene transition boundaries.\n",
    "    - Preliminary scene transition prediction (`scene_transition_boundary_prediction`) is a prediction template indicating the probability of a shot being a scene boundary.\n",
    "    - The `shot_end_frame` is used for evaluation purpose.\n",
    "    \n",
    "Now that we have the data related details out of the way, let us discuss the structure of the rest of this notebook. Section [2](#section2) and [3](#section3) are the typically setup to load the data. Section [4](#section4) go over the data processing and transformations. Section [5](#section5) builds and train LSTM and WaveNet models. Section [6](#section6') discusses hyperparameter tunning and Section [7](#section7) discusses model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Where to get the data\n",
    "PATH = os.path.join(os.getcwd(), \"data_dir\")\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Get the data\n",
    "\n",
    "To load the data, we'll use the `fetch_movies()` function below to unpickle the files and load them into a list of Python dictionaries. Notice the length of the movies ranges from 600 shots to 3100 shots. We will use the maximum length later in the data transformation process so each training instance would have the same shape when fed into tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_movies(path=PATH):\n",
    "    \"\"\"\n",
    "    Load .pkl movie files\n",
    "    \n",
    "    Argument:\n",
    "    ---------\n",
    "    path -- string representing files path\n",
    "    \"\"\"\n",
    "    filenames = glob.glob(os.path.join(PATH, \"tt*.pkl\"))\n",
    "    movies = []\n",
    "    for fn in filenames:\n",
    "        try:\n",
    "            with open(fn, 'rb') as fin:\n",
    "                movies.append(pickle.load(fin))\n",
    "        except EOFError:\n",
    "            break\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = fetch_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and test data sets\n",
    "movies = fetch_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max movie length: 3096\n",
      "Min movie length: 607\n"
     ]
    }
   ],
   "source": [
    "# Movie length\n",
    "movie_lengths = [movie['place'].shape[0] for movie in movies]\n",
    "print(\"Max movie length: {}\".format(max(movie_lengths)))\n",
    "print(\"Min movie length: {}\".format(min(movie_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIM = 2048 + 512 + 512 + 512\n",
    "MAX_MOVIE_LENGTH = 3100\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Data processing\n",
    "\n",
    "Now that we got the data, we will build two custom functions, `split_train_test()` and `transform_movies()` to split the data set into training set and validation set as well as transform them from `torch.Tensor` to numpy arrays. The movies are padded in the transformation process with the `MAX_MOVIE_LENGTH` constant as noted earlier so they all have the same shape. We then call these functions to split and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, train_size=52):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets\n",
    "    \n",
    "    Argument:\n",
    "    --------\n",
    "    data -- a list of dictionaries each containing a movie information\n",
    "    train_size -- integer representing the number of movies used for training\n",
    "    \"\"\"\n",
    "    # For stable output across runs\n",
    "    np.random.seed(42)\n",
    "    # Shuffle indices\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    train_indices = shuffled_indices[:train_size]\n",
    "    test_indices = shuffled_indices[train_size:]\n",
    "    train_set = [data[i] for i in train_indices]\n",
    "    test_set = [data[i] for i in test_indices]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def transform_movies(movies, features=['place', 'cast', 'action', 'audio'], pad_len=MAX_MOVIE_LENGTH):\n",
    "    \"\"\"\n",
    "    Unroll the given features by column and separate features from labels.\n",
    "    Then pad the sequences in each movie to the length of the longest movie.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    movies -- a list of dictionaries each containing a movie information\n",
    "    features -- list of string representing data features\n",
    "    pad-len -- integer for the maximum length of a movie\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    X_padded -- a 2D numpy array\n",
    "    Y_padded -- a 2D numpy array\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    # Unroll the features\n",
    "    for movie in movies: \n",
    "        row = torch.cat([movie[feat] for feat in features], dim=1)\n",
    "        X.append(row.numpy())\n",
    "        # Pre-pad the label since its length is N-1\n",
    "        labels = movie['scene_transition_boundary_ground_truth']\n",
    "        labels = torch.cat([torch.tensor([False]), labels])\n",
    "        Y.append(labels.numpy())\n",
    "    # Pad the sequences\n",
    "    X_padded = pad_sequences(X, maxlen=pad_len, padding='post', dtype='float32')\n",
    "    Y_padded = pad_sequences(Y, value=False, maxlen=pad_len, padding='post')\n",
    "    return X_padded, Y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split movies into training and validation sets\n",
    "movies_train, movies_val = split_train_test(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training and validation sets\n",
    "X_train, y_train = transform_movies(movies_train)\n",
    "X_val, y_val = transform_movies(movies_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Train LSTM and WaveNet\n",
    "\n",
    "Since the training instances are sequences of shots, the logical models would be that of a recurrent neural network (RNN) architecture. However, simpple RNNs do not work well with long sequences. At each time step, the RNN only take the current input and an activation value from the previous time step to make prediction for the current time step. This means a RNN cannot learn from sequences that have long-term dependencies. In particular, if the network is very deep, then the gradient from the output will have a hard time propagating back to affect the earlier layers. That is, as the data traverses the RNN it goes through a series of transformations and after a while, there is very little trace of the first inputs.\n",
    "\n",
    "To combat this problem, we can use __Long Short-Term Memory (LSTM)__ cell which is better at detecting long-term dependencies in the data. It uses update and forget gates that allow the cell to keep more information from the earlier time step compared to RNN. LSTM training is also faster than RNN.\n",
    "\n",
    "Another method to deal with long sequences is the __WaveNet__ architecture introduced in a [2016 paper](https://arxiv.org/abs/1609.03499) by researchers at DeepMind. WaveNet stacks a group of 1D convolutional layers while doubling the dilation rate at every layer. Dilation rate means how far apart each neuron's inputs are. The first layer sees two time steps at a time while the next see four time steps, and so on. In essence, the lower layers in the stack learn short-term patterns while the higher layers learn long-term patterns. This network is extremely fast -- even for long sequences.\n",
    "\n",
    "Before building LSTM and WaveNet, we will build a customized `callback` class which we will pass as argument to the model's `fit()` method. This will allow the training to stop early when accuracy reaches 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"To stop training early once accuracy reach 95%\"\"\"\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy') > 0.95):\n",
    "            print(\"\\nReached 95% accuracy, so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5.1'></a>\n",
    "### 5.1 LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(n_layers=2, n_neurons=32, input_shape=[None, FEATURES_DIM]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for _ in range(n_layers):\n",
    "        model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=n_neurons,\n",
    "                                                               return_sequences=True)))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(1, activation='sigmoid')))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit LSTM classifier\n",
    "lstm_clf = build_lstm()\n",
    "lstm_clf.fit(X_train, y_train, \n",
    "             epochs=NUM_EPOCHS, \n",
    "             callbacks=[myCallback()], \n",
    "             validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5.2'></a>\n",
    "### 5.2 WaveNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wave_net(input_shape=[None, FEATURES_DIM], num_blocks=2, num_layers=4, \n",
    "                   filters1=10, filters2=5, kern1=2, kern2=1, padding='same'):\n",
    "    rates = [2**i for i in range(num_layers)]\n",
    "    wave_model = keras.models.Sequential()\n",
    "    wave_model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for rate in rates * num_blocks:\n",
    "        wave_model.add(keras.layers.Conv1D(filters=filters1, \n",
    "                                              kernel_size=kern1, \n",
    "                                              padding='same',\n",
    "                                              activation='relu', dilation_rate=rate))\n",
    "    wave_model.add(keras.layers.Conv1D(filters=filters2, kernel_size=kern2))\n",
    "    wave_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    wave_model.compile(loss='binary_crossentropy',\n",
    "                       optimizer=keras.optimizers.RMSprop(lr=2e-5),\n",
    "                       metrics=['accuracy'])\n",
    "    return wave_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit WaveNet classifier\n",
    "wave_clf = build_wave_net()\n",
    "wave_clf.fit(X_train, y_train, \n",
    "             epochs=NUM_EPOCHS, \n",
    "             callbacks=[myCallback()], \n",
    "             validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## 6. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the WaveNet architecture while deeper than the simple LSTM, it runs faster. Thus, we will be focusing on tunining the hyperparameter for WaveNet and use it as the final model. We'll take advantage of the `KerasClassifier` wrapper that allows us to use Scikit-Learn's functions with the WaveNet. Additionally, we will tune the hyperparameters using `RandomizedSearchCV` which randomly sample a smaller subset of the hyperparameter space. This is faster than `GridSearchCV` where search is conducted on a larger hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.1'></a>\n",
    "### 6.1 LSTM tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "lstm_keras_clf = KerasClassifier(build_lstm)\n",
    "\n",
    "lstm_params_distribs = {\n",
    "    \"n_layers\": np.arange(1, 11),\n",
    "    \"n_neurons\": [2, 4, 8, 16, 32, 64]\n",
    "}\n",
    "\n",
    "lstm_rnd_search_cv = RandomizedSearchCV(lstm_keras_clf, lstm_params_distribs, n_iter=3, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lstm_rnd_search_cv.fit(X_train, y_train, epochs=NUM_EPOCHS, \n",
    "                       validation_data=(X_val, y_val),\n",
    "                       callbacks=[myCallback()], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.9564256270726522\n",
      "Parameters:\n",
      "\tn_neurons: 2\n",
      "\tn_layers: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: {}\".format(lstm_rnd_search_cv.best_score_))\n",
    "print(\"Parameters:\")\n",
    "for param, value in lstm_rnd_search_cv.best_params_.items():\n",
    "    print(\"\\t{}: {}\".format(param, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.2'></a>\n",
    "### 6.2 WaveNet tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "wave_keras_clf = KerasClassifier(build_wave_net)\n",
    "\n",
    "wave_params_distribs = {\n",
    "    \"num_blocks\": [2, 3],\n",
    "    \"num_layers\": np.arange(4, 11),\n",
    "    \"filters1\": np.arange(10, 21),\n",
    "    \"filters2\": np.arange(2, 11)\n",
    "}\n",
    "\n",
    "wave_rnd_search_cv = RandomizedSearchCV(wave_keras_clf, wave_params_distribs, n_iter=3, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wave_rnd_search_cv.fit(X_train, y_train, epochs=NUM_EPOCHS, \n",
    "                       validation_data=(X_val, y_val),\n",
    "                       callbacks=[myCallback()],\n",
    "                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.9616220394770304\n",
      "Parameters:\n",
      "\tnum_layers: 9\n",
      "\tnum_blocks: 3\n",
      "\tfilters2: 10\n",
      "\tfilters1: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: {}\".format(wave_rnd_search_cv.best_score_))\n",
    "print(\"Parameters:\")\n",
    "for param, value in wave_rnd_search_cv.best_params_.items():\n",
    "    print(\"\\t{}: {}\".format(param, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good sense of the best hyperparameters, let us build a model based on these parameters. First, we will transform the full dataset and fit the best model on this full dataset. Recall that we padded the dataset so that all instances have identical input shape even though the movies are of different length. We will now need to reverse that padding by truncating the predictions so that the predictions for each movie have the same length at the movie itself. Then to write the prediction out to file for evaluation, we will use the `write_predictions()` function.\n",
    "\n",
    "Note that the required model evaluation metrics by Eluv.io are the __Mean Average Precision(mAP)__ and the __Mean Maximum IoU (mean Miou)__. The evaluation script, `evaluate_sceneseg.py` can be found at the Eluv.io's team [Github page](https://github.com/eluv-io/elv-ml-challenge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad_predictions(movies, yhat_probs):\n",
    "    \"\"\"\n",
    "    Truncate the padded predictions to movie's original length\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    movies -- a list of dictionaries containing movies information\n",
    "    yhat_probs -- a 2D numpy array representing prediction for the given movies data set\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    yhat_dict -- a dictionary with each movie imbd_id as key and \n",
    "                 prediction probabilities as a 1D numpy array\n",
    "    \"\"\"\n",
    "    imdb_lengths = [(movie['imdb_id'], movie['place'].shape[0]) for movie in movies]\n",
    "    yhat_dict = dict()\n",
    "    for (imdb, length), yhat in zip(imdb_lengths, yhat_probs):\n",
    "        yhat = yhat[1:length]\n",
    "        yhat_dict[imdb] = yhat\n",
    "    return yhat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(yhat_unpadded_dict, path=PATH):\n",
    "    \"\"\"\n",
    "    Pickle the predictions\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    yhat_unpadded_dict -- a dictionary of prediction consistent with the length of the ground-truth label\n",
    "    path -- a string representing the files path\n",
    "    \"\"\"\n",
    "    for imdb in yhat_unpadded_dict.keys():\n",
    "        # Load existing pkl movie file\n",
    "        filename = os.path.join(PATH, imdb + \".pkl\")\n",
    "        try:\n",
    "            x = pickle.load(open(filename, \"rb\"))\n",
    "            x['scene_transition_boundary_prediction'] = yhat_unpadded_dict[imdb].flatten()\n",
    "            pickle.dump(x, open(filename, \"wb\"))\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, transform the entire dataset \n",
    "X, y = transform_movies(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us rebuild the WaveNet using the best hyperparameters and fit it on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wave_clf_best = build_wave_net(num_layers=9, num_blocks=3, filters1=13, filters2=10)\n",
    "wave_clf_best.fit(X, y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and pickle results\n",
    "yhat_wave = wave_clf_best.predict(X)\n",
    "yhat_wave_unpadded = unpad_predictions(movies, yhat_wave)\n",
    "write_predictions(yhat_wave_unpadded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## 7. Performance evaluation\n",
    "\n",
    "In term of execution speed, WaveNet is much faster than LSTM by order of magnitude -- especially during hyperparameters tunning. __LSTM is prohibitively slow compares to WaveNet!__ Thus, much of the performance evaluation in this section will focus on the WaveNet model.\n",
    "\n",
    "To tune the hyperparameters, a round of random search cross-validation was carried out using tensorflow's `RandomSearchCV` class. This was done using 3-fold cross validation, three iterations, and an early stop criteria of 95% accuracy. Base on the result of the random search, another round of manual tuning was conducted which result in Table 1 below. \n",
    "\n",
    "Let $N$ be the number of blocks in the WaveNet and $L$ be the number of dilation layers in the block, we can derive the following insights:\n",
    "\n",
    "* The number of filters used in the WaveNet does not materially impact performance metrics when $N > 3$ and $L > 4$.\n",
    "* $N$ and $L$ has greater impact on performance metrics than the number of filters.\n",
    "* Miou metric is almost unaffected by changes in hyperparameters.\n",
    "\n",
    "<img src='images/performance.png' width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
